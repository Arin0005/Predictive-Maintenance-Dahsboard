{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3494f37d-b24f-40b2-814c-73b47a866de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 00:27:31.151305: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-27 00:27:31.207755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-27 00:27:32.531779: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal, stats\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf51e52-a34a-4c59-8180-da601b7acf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    CURRENT_TEMP_PATH = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv'\n",
    "    VIBRATION_PATH = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv'\n",
    "    \n",
    "    # Sampling parameters\n",
    "    ORIGINAL_FREQ = 25600  # Hz\n",
    "    TARGET_FREQ = 1000     # Hz\n",
    "    NORMAL_DURATION = 120  # seconds\n",
    "    FAULTY_DURATION = 60   # seconds\n",
    "    \n",
    "    # Window parameters\n",
    "    WINDOW_SIZE = 2000     # 2 seconds at 1kHz\n",
    "    OVERLAP = 0.5          # 50% overlap\n",
    "    \n",
    "    # Fault severity mapping (in mm)\n",
    "    SEVERITY_MAP = {\n",
    "        'Normal': 0.0,\n",
    "        'BPFI_03': 0.3,\n",
    "        'BPFI_10': 1.0,\n",
    "        'BPFI_30': 3.0\n",
    "    }\n",
    "    \n",
    "    # Model parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Synthetic data parameters\n",
    "    NUM_SYNTHETIC_PATHS = 50\n",
    "    AUGMENTATION_FACTOR = 3\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a2776b-2fea-4bfe-b696-df7470717e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_test_data(temp_filepath, vib_filepath, scaler_X, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Load original CSV files and prepare them for model testing\n",
    "    \n",
    "    Args:\n",
    "        temp_filepath: Path to current_temp CSV file\n",
    "        vib_filepath: Path to vibration CSV file\n",
    "        scaler_X: Fitted feature scaler from training\n",
    "        sequence_length: Number of timesteps for LSTM\n",
    "    \n",
    "    Returns:\n",
    "        X_test_scaled: Prepared sequences ready for model prediction\n",
    "        feature_df: Extracted features (for inspection)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LOADING AND PREPARING TEST DATA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Step 1: Load CSV files\n",
    "    print(f\"\\n[1/5] Loading CSV files...\")\n",
    "    df_temp = pd.read_csv(temp_filepath)\n",
    "    df_vib = pd.read_csv(vib_filepath)\n",
    "    print(f\"✓ Temperature data: {df_temp.shape}\")\n",
    "    print(f\"✓ Vibration data: {df_vib.shape}\")\n",
    "    \n",
    "    # Step 2: Preprocess (resample from 25.6kHz to 1kHz)\n",
    "    print(f\"\\n[2/5] Resampling from {config.ORIGINAL_FREQ}Hz to {config.TARGET_FREQ}Hz...\")\n",
    "    df_temp_resampled = preprocess_dataframe(df_temp, 'current_temp')\n",
    "    df_vib_resampled = preprocess_dataframe(df_vib, 'vibration')\n",
    "    print(f\"✓ Temperature resampled: {df_temp_resampled.shape}\")\n",
    "    print(f\"✓ Vibration resampled: {df_vib_resampled.shape}\")\n",
    "    \n",
    "    # Step 3: Extract features from windows\n",
    "    print(f\"\\n[3/5] Extracting features from {config.WINDOW_SIZE} sample windows...\")\n",
    "    feature_df = create_windowed_features(\n",
    "        df_temp_resampled, \n",
    "        df_vib_resampled, \n",
    "        config.WINDOW_SIZE, \n",
    "        config.OVERLAP\n",
    "    )\n",
    "    print(f\"✓ Extracted features: {feature_df.shape}\")\n",
    "    print(f\"  Number of windows: {feature_df.shape[0]}\")\n",
    "    print(f\"  Number of features per window: {feature_df.shape[1]}\")\n",
    "    \n",
    "    # Step 4: Create sequences for LSTM\n",
    "    print(f\"\\n[4/5] Creating sequences of length {sequence_length}...\")\n",
    "    X = feature_df.values\n",
    "    \n",
    "    # Create sequences (sliding window approach)\n",
    "    X_sequences = []\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "    \n",
    "    X_sequences = np.array(X_sequences)\n",
    "    print(f\"✓ Created {X_sequences.shape[0]} sequences\")\n",
    "    print(f\"  Sequence shape: {X_sequences.shape}\")\n",
    "    \n",
    "    # Step 5: Scale features using the same scaler from training\n",
    "    print(f\"\\n[5/5] Scaling features...\")\n",
    "    X_reshaped = X_sequences.reshape(-1, X_sequences.shape[-1])\n",
    "    X_scaled = scaler_X.transform(X_reshaped)\n",
    "    X_test_scaled = X_scaled.reshape(X_sequences.shape)\n",
    "    print(f\"✓ Features scaled\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DATA PREPARATION COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Ready for prediction: {X_test_scaled.shape}\")\n",
    "    \n",
    "    return X_test_scaled, feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92caffa1-ddb3-4f0b-a77d-308d344f944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_original_data(temp_filepath, vib_filepath, \n",
    "                                  model_path='predictive_maintenance_model.h5',\n",
    "                                  scaler_X_path='scaler_X.pkl',\n",
    "                                  scaler_y_path='scaler_y.pkl',\n",
    "                                  sequence_length=10):\n",
    "    \"\"\"\n",
    "    Complete pipeline to test trained model on original data\n",
    "    \n",
    "    Args:\n",
    "        temp_filepath: Path to temperature CSV\n",
    "        vib_filepath: Path to vibration CSV\n",
    "        model_path: Path to saved model\n",
    "        scaler_X_path: Path to feature scaler\n",
    "        scaler_y_path: Path to target scaler\n",
    "        sequence_length: LSTM sequence length\n",
    "    \n",
    "    Returns:\n",
    "        predictions_df: DataFrame with all predictions and analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TESTING MODEL ON ORIGINAL DATA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Load model and scalers\n",
    "    print(\"\\n[1/3] Loading trained model and scalers...\")\n",
    "    try:\n",
    "        model = keras.models.load_model(model_path,compile=False)\n",
    "        scaler_X = joblib.load(scaler_X_path)\n",
    "        scaler_y = joblib.load(scaler_y_path)\n",
    "        print(\"✓ Model and scalers loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading model/scalers: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(\"\\n[2/3] Preparing test data...\")\n",
    "    X_test_scaled, feature_df = load_and_prepare_test_data(\n",
    "        temp_filepath, \n",
    "        vib_filepath, \n",
    "        scaler_X, \n",
    "        sequence_length\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\n[3/3] Making predictions...\")\n",
    "    y_pred_scaled = model.predict(X_test_scaled, verbose=1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "    \n",
    "    print(f\"✓ Generated {len(y_pred)} predictions\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = {\n",
    "        'sequence_index': np.arange(len(y_pred)),\n",
    "        'predicted_RUL_minutes': y_pred,\n",
    "        'predicted_RUL_hours': y_pred / 60,\n",
    "        'health_state': ['Healthy' if r > 60 else 'Warning' if r > 20 else 'Critical' for r in y_pred],\n",
    "        'maintenance_urgency': ['Low' if r > 60 else 'Medium' if r > 20 else 'High' for r in y_pred]\n",
    "    }\n",
    "    \n",
    "    predictions_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total sequences analyzed: {len(y_pred)}\")\n",
    "    print(f\"\\nRUL Statistics:\")\n",
    "    print(f\"  Mean RUL: {y_pred.mean():.2f} minutes ({y_pred.mean()/60:.2f} hours)\")\n",
    "    print(f\"  Min RUL: {y_pred.min():.2f} minutes\")\n",
    "    print(f\"  Max RUL: {y_pred.max():.2f} minutes\")\n",
    "    print(f\"  Std Dev: {y_pred.std():.2f} minutes\")\n",
    "    \n",
    "    print(f\"\\nHealth State Distribution:\")\n",
    "    print(predictions_df['health_state'].value_counts())\n",
    "    \n",
    "    # Identify critical sequences\n",
    "    critical_sequences = predictions_df[predictions_df['health_state'] == 'Critical']\n",
    "    if len(critical_sequences) > 0:\n",
    "        print(f\"\\n⚠️  WARNING: {len(critical_sequences)} CRITICAL sequences detected!\")\n",
    "        print(f\"   Minimum RUL: {critical_sequences['predicted_RUL_minutes'].min():.2f} minutes\")\n",
    "        print(f\"   Action: IMMEDIATE MAINTENANCE REQUIRED!\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No critical conditions detected\")\n",
    "    \n",
    "    # Save results\n",
    "    output_filename = 'test_predictions.csv'\n",
    "    predictions_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\n✓ Predictions saved to '{output_filename}'\")\n",
    "    \n",
    "    return predictions_df, y_pred, feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b71e87c8-605d-4000-a13e-01dfe3d924eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test_predictions(predictions_df, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize predictions on test data\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with prediction results\n",
    "        y_pred: Array of predicted RUL values\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. RUL over time\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(predictions_df['sequence_index'], predictions_df['predicted_RUL_minutes'], \n",
    "             linewidth=2, color='blue', alpha=0.7)\n",
    "    ax1.axhline(60, color='green', linestyle='--', linewidth=2, label='Healthy Threshold')\n",
    "    ax1.axhline(20, color='red', linestyle='--', linewidth=2, label='Critical Threshold')\n",
    "    ax1.fill_between(predictions_df['sequence_index'], 0, 20, alpha=0.2, color='red')\n",
    "    ax1.fill_between(predictions_df['sequence_index'], 20, 60, alpha=0.2, color='orange')\n",
    "    ax1.fill_between(predictions_df['sequence_index'], 60, predictions_df['predicted_RUL_minutes'].max(), \n",
    "                     alpha=0.2, color='green')\n",
    "    ax1.set_xlabel('Sequence Index', fontsize=12)\n",
    "    ax1.set_ylabel('Predicted RUL (minutes)', fontsize=12)\n",
    "    ax1.set_title('RUL Progression Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Health state distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    health_counts = predictions_df['health_state'].value_counts()\n",
    "    colors = ['#22c55e' if state == 'Healthy' else '#f59e0b' if state == 'Warning' else '#ef4444' \n",
    "              for state in health_counts.index]\n",
    "    bars = ax2.bar(health_counts.index, health_counts.values, color=colors, edgecolor='black')\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('Health State Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, count in zip(bars, health_counts.values):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count}\\n({count/len(predictions_df)*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 3. RUL histogram\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.hist(y_pred, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax3.axvline(y_pred.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {y_pred.mean():.1f} min')\n",
    "    ax3.axvline(60, color='green', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax3.axvline(20, color='red', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax3.set_xlabel('Predicted RUL (minutes)', fontsize=12)\n",
    "    ax3.set_ylabel('Frequency', fontsize=12)\n",
    "    ax3.set_title('RUL Distribution', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Moving average\n",
    "    ax4 = axes[1, 1]\n",
    "    window = min(10, len(y_pred) // 10)\n",
    "    if window > 0:\n",
    "        moving_avg = pd.Series(y_pred).rolling(window=window, min_periods=1).mean()\n",
    "        ax4.plot(predictions_df['sequence_index'], y_pred, \n",
    "                alpha=0.3, color='blue', label='Raw Predictions')\n",
    "        ax4.plot(predictions_df['sequence_index'], moving_avg, \n",
    "                linewidth=2, color='darkblue', label=f'Moving Avg (window={window})')\n",
    "        ax4.axhline(20, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "        ax4.set_xlabel('Sequence Index', fontsize=12)\n",
    "        ax4.set_ylabel('RUL (minutes)', fontsize=12)\n",
    "        ax4.set_title('RUL with Moving Average', fontsize=14, fontweight='bold')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_predictions_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Visualization saved as 'test_predictions_visualization.png'\")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df, data_type='current_temp'):\n",
    "    \"\"\"Preprocess dataframe: resample and clean\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Remove timestamp column if exists\n",
    "    if 'timestamp' in df_processed.columns:\n",
    "        df_processed = df_processed.drop('timestamp', axis=1)\n",
    "    \n",
    "    # Resample all columns\n",
    "    resampled_data = {}\n",
    "    for col in df_processed.columns:\n",
    "        resampled_data[col] = resample_signal(\n",
    "            df_processed[col].values,\n",
    "            config.ORIGINAL_FREQ,\n",
    "            config.TARGET_FREQ\n",
    "        )\n",
    "    \n",
    "    df_resampled = pd.DataFrame(resampled_data)\n",
    "    \n",
    "    # Handle NaN and infinite values\n",
    "    df_resampled = df_resampled.replace([np.inf, -np.inf], np.nan)\n",
    "    df_resampled = df_resampled.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "def resample_signal(signal_data, original_freq, target_freq):\n",
    "    \"\"\"Resample signal from original frequency to target frequency\"\"\"\n",
    "    num_samples = len(signal_data)\n",
    "    duration = num_samples / original_freq\n",
    "    target_samples = int(duration * target_freq)\n",
    "    \n",
    "    # Use scipy's resample for high-quality resampling\n",
    "    resampled = signal.resample(signal_data, target_samples)\n",
    "    return resampled\n",
    "\n",
    "def create_windowed_features(df_temp, df_vib, window_size, overlap):\n",
    "    \"\"\"Create windowed features from sensor data\"\"\"\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    num_windows = (len(df_temp) - window_size) // step_size + 1\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * step_size\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        if end_idx > len(df_temp):\n",
    "            break\n",
    "        \n",
    "        window_features = {}\n",
    "        \n",
    "        # Temperature features\n",
    "        for col in df_temp.columns:\n",
    "            temp_window = df_temp[col].values[start_idx:end_idx]\n",
    "            time_feats = extract_time_domain_features(temp_window)\n",
    "            for feat_name, feat_val in time_feats.items():\n",
    "                window_features[f'temp_{col}_{feat_name}'] = feat_val\n",
    "        \n",
    "        # Vibration features\n",
    "        for col in df_vib.columns:\n",
    "            vib_window = df_vib[col].values[start_idx:end_idx]\n",
    "            time_feats = extract_time_domain_features(vib_window)\n",
    "            freq_feats = extract_frequency_domain_features(vib_window)\n",
    "            for feat_name, feat_val in {**time_feats, **freq_feats}.items():\n",
    "                window_features[f'vib_{col}_{feat_name}'] = feat_val\n",
    "        \n",
    "        features_list.append(window_features)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "\n",
    "def extract_time_domain_features(window):\n",
    "    \"\"\"Extract time-domain features from a signal window\"\"\"\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(window)\n",
    "    features['std'] = np.std(window)\n",
    "    features['rms'] = np.sqrt(np.mean(window**2))\n",
    "    features['peak'] = np.max(np.abs(window))\n",
    "    features['peak_to_peak'] = np.ptp(window)\n",
    "    features['crest_factor'] = features['peak'] / (features['rms'] + 1e-8)\n",
    "    features['kurtosis'] = stats.kurtosis(window)\n",
    "    features['skewness'] = stats.skew(window)\n",
    "    return features\n",
    "\n",
    "def extract_frequency_domain_features(window, fs=1000):\n",
    "    \"\"\"Extract frequency-domain features from a signal window\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # FFT\n",
    "    fft_vals = np.fft.fft(window)\n",
    "    fft_freq = np.fft.fftfreq(len(window), 1/fs)\n",
    "    fft_power = np.abs(fft_vals)**2\n",
    "    \n",
    "    # Only positive frequencies\n",
    "    pos_mask = fft_freq > 0\n",
    "    fft_freq_pos = fft_freq[pos_mask]\n",
    "    fft_power_pos = fft_power[pos_mask]\n",
    "    \n",
    "    features['spectral_centroid'] = np.sum(fft_freq_pos * fft_power_pos) / (np.sum(fft_power_pos) + 1e-8)\n",
    "    features['spectral_variance'] = np.sqrt(np.sum(((fft_freq_pos - features['spectral_centroid'])**2) * fft_power_pos) / (np.sum(fft_power_pos) + 1e-8))\n",
    "    \n",
    "    # Bearing fault frequencies (example for BPFI)\n",
    "    bpfi_range = (fft_freq_pos >= 100) & (fft_freq_pos <= 200)\n",
    "    features['bpfi_energy'] = np.sum(fft_power_pos[bpfi_range])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a092d7d-4399-4e46-865b-14ad2ba82cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_test_single_file():\n",
    "    \"\"\"\n",
    "    Example: Test model on a single pair of files\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE: Testing on Single File\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Specify your file paths\n",
    "    temp_file = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/0Nm_Normal.csv'  # Change to your file\n",
    "    vib_file = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/0Nm_Normal.csv'      # Change to your file\n",
    "    \n",
    "    # Test the model\n",
    "    predictions_df, y_pred, features = test_model_on_original_data(\n",
    "        temp_filepath=temp_file,\n",
    "        vib_filepath=vib_file,\n",
    "        model_path = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/predictive_maintenance_model.h5',\n",
    "        scaler_X_path = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/scaler_X.pkl',\n",
    "        scaler_y_path = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/scaler_y.pkl',\n",
    "        sequence_length=10)\n",
    "    \n",
    "    # Visualize results\n",
    "    if predictions_df is not None:\n",
    "        visualize_test_predictions(predictions_df, y_pred)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "\n",
    "def example_test_multiple_files():\n",
    "    \"\"\"\n",
    "    Example: Test model on multiple files\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE: Testing on Multiple Files\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "    temp_file = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/'  # Change to your file\n",
    "    vib_file = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/'      # Change to your file\n",
    "    \n",
    "    # Define all test files\n",
    "    test_files = [\n",
    "    ('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/0Nm_Normal.csv',\n",
    "     '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/0Nm_Normal.csv',\n",
    "     'Normal'),\n",
    "\n",
    "    ('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/0Nm_BPFI_03.csv',\n",
    "     '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/0Nm_BPFI_03.csv',\n",
    "     'BPFI_03'),\n",
    "\n",
    "    ('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/0Nm_BPFI_10.csv',\n",
    "     '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/0Nm_BPFI_10.csv',\n",
    "     'BPFI_10'),\n",
    "\n",
    "    ('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/0Nm_BPFI_30.csv',\n",
    "     '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/0Nm_BPFI_30.csv',\n",
    "     'BPFI_30'),\n",
    "                ]\n",
    "\n",
    "    \n",
    "    # Load model and scalers once\n",
    "    model = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/predictive_maintenance_model.h5'\n",
    "    scaler_X = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/scaler_X.pkl'\n",
    "    scaler_y = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/scaler_y.pkl'\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for temp_file, vib_file, condition in test_files:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Testing: {condition}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            predictions_df, y_pred, _ = test_model_on_original_data(\n",
    "                temp_file, vib_file,\n",
    "                model_path=model,\n",
    "                scaler_X_path=scaler_X,\n",
    "                scaler_y_path=scaler_y\n",
    "            )\n",
    "            \n",
    "            all_results[condition] = {\n",
    "                'predictions': predictions_df,\n",
    "                'mean_rul': y_pred.mean(),\n",
    "                'min_rul': y_pred.min(),\n",
    "                'max_rul': y_pred.max()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error testing {condition}: {e}\")\n",
    "    \n",
    "    # Compare results across conditions\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COMPARISON ACROSS CONDITIONS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    comparison_data = []\n",
    "    for condition, results in all_results.items():\n",
    "        comparison_data.append({\n",
    "            'Condition': condition,\n",
    "            'Mean RUL (min)': results['mean_rul'],\n",
    "            'Min RUL (min)': results['min_rul'],\n",
    "            'Max RUL (min)': results['max_rul']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    comparison_df.to_csv('comparison_results.csv', index=False)\n",
    "    print(f\"\\n✓ Comparison saved to 'comparison_results.csv'\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def example_real_time_monitoring():\n",
    "    \"\"\"\n",
    "    Example: Simulate real-time monitoring\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE: Real-Time Monitoring Simulation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    temp_file = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0NM_current_temp_csv/0Nm_Normal.csv'  # Change to your file\n",
    "    vib_file = '/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/0nm_vibration_csv/0Nm_Normal.csv'      # Change to your file\n",
    "    \n",
    "    # Load and prepare data\n",
    "    model = keras.models.load_model('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/predictive_maintenance_model.h5',compile=False)\n",
    "    scaler_X = joblib.load('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/scaler_X.pkl')\n",
    "    scaler_y = joblib.load('/media/harish/bc5d683c-2fdb-46e5-8916-190976536d13/Major project/Datasets/iteration 3/CNN_LSTM model/scaler_y.pkl')\n",
    "    \n",
    "    X_test_scaled, _ = load_and_prepare_test_data(temp_file, vib_file, scaler_X)\n",
    "    \n",
    "    # Simulate real-time predictions (every 10 sequences)\n",
    "    print(\"\\nSimulating real-time monitoring...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(0, len(X_test_scaled), 10):\n",
    "        sequence = X_test_scaled[i:i+1]\n",
    "        \n",
    "        rul_scaled = model.predict(sequence, verbose=0)\n",
    "        rul = scaler_y.inverse_transform(rul_scaled).flatten()[0]\n",
    "        \n",
    "        if rul > 60:\n",
    "            status = \"✓ HEALTHY\"\n",
    "            color = '\\033[92m'  # Green\n",
    "        elif rul > 20:\n",
    "            status = \"⚠ WARNING\"\n",
    "            color = '\\033[93m'  # Yellow\n",
    "        else:\n",
    "            status = \"✗ CRITICAL\"\n",
    "            color = '\\033[91m'  # Red\n",
    "        \n",
    "        print(f\"{color}Sequence {i:4d} | RUL: {rul:6.2f} min | {status}\\033[0m\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38785d8-0d61-4415-bd9a-fe3483a07d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE: Real-Time Monitoring Simulation\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "LOADING AND PREPARING TEST DATA\n",
      "======================================================================\n",
      "\n",
      "[1/5] Loading CSV files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 00:27:56.301130: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Temperature data: (7682458, 6)\n",
      "✓ Vibration data: (7680000, 5)\n",
      "\n",
      "[2/5] Resampling from 25600Hz to 1000Hz...\n",
      "✓ Temperature resampled: (300096, 6)\n",
      "✓ Vibration resampled: (300000, 5)\n",
      "\n",
      "[3/5] Extracting features from 2000 sample windows...\n",
      "✓ Extracted features: (299, 103)\n",
      "  Number of windows: 299\n",
      "  Number of features per window: 103\n",
      "\n",
      "[4/5] Creating sequences of length 10...\n",
      "✓ Created 290 sequences\n",
      "  Sequence shape: (290, 10, 103)\n",
      "\n",
      "[5/5] Scaling features...\n",
      "✓ Features scaled\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION COMPLETE!\n",
      "======================================================================\n",
      "Ready for prediction: (290, 10, 103)\n",
      "\n",
      "Simulating real-time monitoring...\n",
      "----------------------------------------------------------------------\n",
      "\u001b[92mSequence    0 | RUL:  88.03 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   10 | RUL:  80.85 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   20 | RUL:  88.65 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   30 | RUL:  82.58 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   40 | RUL:  86.97 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   50 | RUL:  81.49 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   60 | RUL:  89.07 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   70 | RUL:  79.71 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   80 | RUL:  67.15 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence   90 | RUL:  87.50 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  100 | RUL:  82.65 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  110 | RUL:  78.83 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  120 | RUL:  68.01 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  130 | RUL:  75.96 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  140 | RUL:  76.13 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  150 | RUL:  66.10 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  160 | RUL:  72.00 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  170 | RUL:  71.05 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  180 | RUL:  73.47 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  190 | RUL:  72.23 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  200 | RUL:  77.79 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  210 | RUL:  73.15 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  220 | RUL:  75.57 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  230 | RUL:  69.32 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  240 | RUL:  65.66 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  250 | RUL:  70.98 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  260 | RUL:  79.52 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  270 | RUL:  71.13 min | ✓ HEALTHY\u001b[0m\n",
      "\u001b[92mSequence  280 | RUL:  70.77 min | ✓ HEALTHY\u001b[0m\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_real_time_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338ec47-20b7-415a-8cd8-36fbe5df6778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
