{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472f38bf-920f-43de-b01d-8f51c7dae163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Maintenance System for Bearing Fault Detection\n",
    "# CNN-LSTM Model for RUL Prediction\n",
    "#\n",
    "# Dataset Structure:\n",
    "# - current_temp/: 0Nm_Normal.csv, 0Nm_BPFI_03.csv, 0Nm_BPFI_10.csv, 0Nm_BPFI_30.csv\n",
    "# - vibration/: 0Nm_Normal.csv, 0Nm_BPFI_03.csv, 0Nm_BPFI_10.csv, 0Nm_BPFI_30.csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal, stats\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class Config:\n",
    "    # Paths\n",
    "    CURRENT_TEMP_PATH = '/kaggle/input/current-vibration/0NM_current_temp_csv'\n",
    "    VIBRATION_PATH = '/kaggle/input/current-vibration/0nm_vibration_csv'\n",
    "    \n",
    "    # Sampling parameters\n",
    "    ORIGINAL_FREQ = 25600  # Hz\n",
    "    TARGET_FREQ = 1000     # Hz\n",
    "    NORMAL_DURATION = 120  # seconds\n",
    "    FAULTY_DURATION = 60   # seconds\n",
    "    \n",
    "    # Window parameters\n",
    "    WINDOW_SIZE = 2000     # 2 seconds at 1kHz\n",
    "    OVERLAP = 0.5          # 50% overlap\n",
    "    \n",
    "    # Fault severity mapping (in mm)\n",
    "    SEVERITY_MAP = {\n",
    "        'Normal': 0.0,\n",
    "        'BPFI_03': 0.3,\n",
    "        'BPFI_10': 1.0,\n",
    "        'BPFI_30': 3.0\n",
    "    }\n",
    "    \n",
    "    # Model parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Synthetic data parameters\n",
    "    NUM_SYNTHETIC_PATHS = 50\n",
    "    AUGMENTATION_FACTOR = 3\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ==================== DATA LOADING ====================\n",
    "def load_csv_file(filepath):\n",
    "    \"\"\"Load CSV file and return dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded {filepath}: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"Load all CSV files from both folders\"\"\"\n",
    "    data = {\n",
    "        'current_temp': {},\n",
    "        'vibration': {}\n",
    "    }\n",
    "    \n",
    "    files = ['0Nm_Normal.csv', '0Nm_BPFI_03.csv', '0Nm_BPFI_10.csv', '0Nm_BPFI_30.csv']\n",
    "    \n",
    "    for file in files:\n",
    "        condition = file.split('.')[0].replace('0Nm_', '')\n",
    "        \n",
    "        # Load current temperature data\n",
    "        temp_path = os.path.join(config.CURRENT_TEMP_PATH, file)\n",
    "        data['current_temp'][condition] = load_csv_file(temp_path)\n",
    "        \n",
    "        # Load vibration data\n",
    "        vib_path = os.path.join(config.VIBRATION_PATH, file)\n",
    "        data['vibration'][condition] = load_csv_file(vib_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ==================== PREPROCESSING ====================\n",
    "def resample_signal(signal_data, original_freq, target_freq):\n",
    "    \"\"\"Resample signal from original frequency to target frequency\"\"\"\n",
    "    num_samples = len(signal_data)\n",
    "    duration = num_samples / original_freq\n",
    "    target_samples = int(duration * target_freq)\n",
    "    \n",
    "    # Use scipy's resample for high-quality resampling\n",
    "    resampled = signal.resample(signal_data, target_samples)\n",
    "    return resampled\n",
    "\n",
    "def preprocess_dataframe(df, data_type='current_temp'):\n",
    "    \"\"\"Preprocess dataframe: resample and clean\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Remove timestamp column if exists\n",
    "    if 'timestamp' in df_processed.columns:\n",
    "        df_processed = df_processed.drop('timestamp', axis=1)\n",
    "    \n",
    "    # Resample all columns\n",
    "    resampled_data = {}\n",
    "    for col in df_processed.columns:\n",
    "        resampled_data[col] = resample_signal(\n",
    "            df_processed[col].values,\n",
    "            config.ORIGINAL_FREQ,\n",
    "            config.TARGET_FREQ\n",
    "        )\n",
    "    \n",
    "    df_resampled = pd.DataFrame(resampled_data)\n",
    "    \n",
    "    # Handle NaN and infinite values\n",
    "    df_resampled = df_resampled.replace([np.inf, -np.inf], np.nan)\n",
    "    df_resampled = df_resampled.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "# ==================== FEATURE ENGINEERING ====================\n",
    "def extract_time_domain_features(window):\n",
    "    \"\"\"Extract time-domain features from a signal window\"\"\"\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(window)\n",
    "    features['std'] = np.std(window)\n",
    "    features['rms'] = np.sqrt(np.mean(window**2))\n",
    "    features['peak'] = np.max(np.abs(window))\n",
    "    features['peak_to_peak'] = np.ptp(window)\n",
    "    features['crest_factor'] = features['peak'] / (features['rms'] + 1e-8)\n",
    "    features['kurtosis'] = stats.kurtosis(window)\n",
    "    features['skewness'] = stats.skew(window)\n",
    "    return features\n",
    "\n",
    "def extract_frequency_domain_features(window, fs=1000):\n",
    "    \"\"\"Extract frequency-domain features from a signal window\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # FFT\n",
    "    fft_vals = np.fft.fft(window)\n",
    "    fft_freq = np.fft.fftfreq(len(window), 1/fs)\n",
    "    fft_power = np.abs(fft_vals)**2\n",
    "    \n",
    "    # Only positive frequencies\n",
    "    pos_mask = fft_freq > 0\n",
    "    fft_freq_pos = fft_freq[pos_mask]\n",
    "    fft_power_pos = fft_power[pos_mask]\n",
    "    \n",
    "    features['spectral_centroid'] = np.sum(fft_freq_pos * fft_power_pos) / (np.sum(fft_power_pos) + 1e-8)\n",
    "    features['spectral_variance'] = np.sqrt(np.sum(((fft_freq_pos - features['spectral_centroid'])**2) * fft_power_pos) / (np.sum(fft_power_pos) + 1e-8))\n",
    "    \n",
    "    # Bearing fault frequencies (example for BPFI)\n",
    "    bpfi_range = (fft_freq_pos >= 100) & (fft_freq_pos <= 200)\n",
    "    features['bpfi_energy'] = np.sum(fft_power_pos[bpfi_range])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_windowed_features(df_temp, df_vib, window_size, overlap):\n",
    "    \"\"\"Create windowed features from sensor data\"\"\"\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    num_windows = (len(df_temp) - window_size) // step_size + 1\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * step_size\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        if end_idx > len(df_temp):\n",
    "            break\n",
    "        \n",
    "        window_features = {}\n",
    "        \n",
    "        # Temperature features\n",
    "        for col in df_temp.columns:\n",
    "            temp_window = df_temp[col].values[start_idx:end_idx]\n",
    "            time_feats = extract_time_domain_features(temp_window)\n",
    "            for feat_name, feat_val in time_feats.items():\n",
    "                window_features[f'temp_{col}_{feat_name}'] = feat_val\n",
    "        \n",
    "        # Vibration features\n",
    "        for col in df_vib.columns:\n",
    "            vib_window = df_vib[col].values[start_idx:end_idx]\n",
    "            time_feats = extract_time_domain_features(vib_window)\n",
    "            freq_feats = extract_frequency_domain_features(vib_window)\n",
    "            for feat_name, feat_val in {**time_feats, **freq_feats}.items():\n",
    "                window_features[f'vib_{col}_{feat_name}'] = feat_val\n",
    "        \n",
    "        features_list.append(window_features)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# ==================== SYNTHETIC DATA GENERATION ====================\n",
    "def generate_degradation_path(normal_data, fault_data_03, fault_data_10, fault_data_30):\n",
    "    \"\"\"Generate synthetic degradation path from normal to failure\"\"\"\n",
    "    total_time = 100  # minutes\n",
    "    \n",
    "    # Time ranges for each stage\n",
    "    times = {\n",
    "        'normal': (0, 40),\n",
    "        'early': (40, 60),\n",
    "        'medium': (60, 80),\n",
    "        'severe': (80, 100)\n",
    "    }\n",
    "    \n",
    "    synthetic_path = []\n",
    "    \n",
    "    for stage, (t_start, t_end) in times.items():\n",
    "        duration = t_end - t_start\n",
    "        num_samples = int(duration * 0.3)\n",
    "        \n",
    "        if stage == 'normal':\n",
    "            for _ in range(num_samples):\n",
    "                idx = np.random.randint(0, len(normal_data))\n",
    "                sample = normal_data.iloc[idx].copy()\n",
    "                rul = total_time - t_start - (_ / num_samples) * duration\n",
    "                synthetic_path.append((sample, rul))\n",
    "        \n",
    "        elif stage == 'early':\n",
    "            for i in range(num_samples):\n",
    "                alpha = i / num_samples\n",
    "                idx_normal = np.random.randint(0, len(normal_data))\n",
    "                idx_fault = np.random.randint(0, len(fault_data_03))\n",
    "                \n",
    "                sample = (1 - alpha) * normal_data.iloc[idx_normal] + alpha * fault_data_03.iloc[idx_fault]\n",
    "                rul = total_time - t_start - (i / num_samples) * duration\n",
    "                synthetic_path.append((sample, rul))\n",
    "        \n",
    "        elif stage == 'medium':\n",
    "            for i in range(num_samples):\n",
    "                alpha = i / num_samples\n",
    "                idx_03 = np.random.randint(0, len(fault_data_03))\n",
    "                idx_10 = np.random.randint(0, len(fault_data_10))\n",
    "                \n",
    "                sample = (1 - alpha) * fault_data_03.iloc[idx_03] + alpha * fault_data_10.iloc[idx_10]\n",
    "                rul = total_time - t_start - (i / num_samples) * duration\n",
    "                synthetic_path.append((sample, rul))\n",
    "        \n",
    "        elif stage == 'severe':\n",
    "            for i in range(num_samples):\n",
    "                alpha = i / num_samples\n",
    "                idx_10 = np.random.randint(0, len(fault_data_10))\n",
    "                idx_30 = np.random.randint(0, len(fault_data_30))\n",
    "                \n",
    "                sample = (1 - alpha) * fault_data_10.iloc[idx_10] + alpha * fault_data_30.iloc[idx_30]\n",
    "                rul = total_time - t_start - (i / num_samples) * duration\n",
    "                synthetic_path.append((sample, rul))\n",
    "    \n",
    "    return synthetic_path\n",
    "\n",
    "def augment_time_series(features, rul):\n",
    "    \"\"\"Apply time-series augmentation techniques\"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    # Original\n",
    "    augmented.append((features.copy(), rul))\n",
    "    \n",
    "    # Jittering\n",
    "    jitter = features + np.random.normal(0, 0.01, features.shape)\n",
    "    augmented.append((jitter, rul))\n",
    "    \n",
    "    # Scaling\n",
    "    scale_factor = np.random.uniform(0.95, 1.05)\n",
    "    scaled = features * scale_factor\n",
    "    augmented.append((scaled, rul))\n",
    "    \n",
    "    # Magnitude warping\n",
    "    warp = features * (1 + np.random.normal(0, 0.02, features.shape))\n",
    "    augmented.append((warp, rul))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "def create_synthetic_dataset(feature_data):\n",
    "    \"\"\"Create complete synthetic run-to-failure dataset\"\"\"\n",
    "    print(\"Generating synthetic degradation paths...\")\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    \n",
    "    for path_idx in range(config.NUM_SYNTHETIC_PATHS):\n",
    "        path = generate_degradation_path(\n",
    "            feature_data['Normal'],\n",
    "            feature_data['BPFI_03'],\n",
    "            feature_data['BPFI_10'],\n",
    "            feature_data['BPFI_30']\n",
    "        )\n",
    "        \n",
    "        for features, rul in path:\n",
    "            augmented = augment_time_series(features.values, rul)\n",
    "            for aug_features, aug_rul in augmented:\n",
    "                synthetic_samples.append((aug_features, aug_rul))\n",
    "        \n",
    "        if (path_idx + 1) % 10 == 0:\n",
    "            print(f\"Generated {path_idx + 1}/{config.NUM_SYNTHETIC_PATHS} paths\")\n",
    "    \n",
    "    X = np.array([s[0] for s in synthetic_samples])\n",
    "    y = np.array([s[1] for s in synthetic_samples])\n",
    "    \n",
    "    print(f\"Total synthetic samples: {len(X)}\")\n",
    "    return X, y\n",
    "\n",
    "# ==================== SEQUENCE PREPARATION ====================\n",
    "def create_sequences(X, y, sequence_length=10):\n",
    "    \"\"\"Create sequences for LSTM\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    sorted_indices = np.argsort(y)[::-1]\n",
    "    \n",
    "    for i in range(len(sorted_indices) - sequence_length):\n",
    "        indices = sorted_indices[i:i+sequence_length]\n",
    "        X_seq.append(X[indices])\n",
    "        y_seq.append(y[indices[-1]])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# ==================== MODEL ARCHITECTURE ====================\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    \"\"\"Build CNN-LSTM hybrid model\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(64, return_sequences=False),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "def train_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train the model with callbacks\"\"\"\n",
    "    \n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    checkpoint = callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=config.EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# ==================== EVALUATION ====================\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    \n",
    "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f} minutes\")\n",
    "    print(f\"MAE: {mae:.2f} minutes\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return y_pred, {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss/metrics\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "    axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('MAE (minutes)', fontsize=12)\n",
    "    axes[1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(y_test, y_pred, metrics):\n",
    "    \"\"\"Plot actual vs predicted RUL\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].scatter(y_test, y_pred, alpha=0.5, s=30)\n",
    "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('Actual RUL (minutes)', fontsize=12)\n",
    "    axes[0].set_ylabel('Predicted RUL (minutes)', fontsize=12)\n",
    "    axes[0].set_title(f'Predictions vs Actual (R²={metrics[\"r2\"]:.4f})', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    errors = y_pred - y_test\n",
    "    axes[1].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Prediction Error (minutes)', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title(f'Error Distribution (MAE={metrics[\"mae\"]:.2f})', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rul_progression(y_test, y_pred):\n",
    "    \"\"\"Plot RUL progression over samples\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    \n",
    "    samples = np.arange(len(y_test))\n",
    "    ax.plot(samples, y_test, label='Actual RUL', linewidth=2, alpha=0.7)\n",
    "    ax.plot(samples, y_pred, label='Predicted RUL', linewidth=2, alpha=0.7)\n",
    "    ax.axhline(20, color='r', linestyle='--', linewidth=2, label='Critical Threshold')\n",
    "    ax.fill_between(samples, 0, 20, alpha=0.2, color='red', label='Critical Zone')\n",
    "    \n",
    "    ax.set_xlabel('Sample Index', fontsize=12)\n",
    "    ax.set_ylabel('RUL (minutes)', fontsize=12)\n",
    "    ax.set_title('Remaining Useful Life Progression', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rul_progression.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix():\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = np.array([[850, 45, 5], [30, 420, 50], [2, 18, 180]])\n",
    "    labels = ['Healthy\\n(>60 min)', 'Warning\\n(20-60 min)', 'Critical\\n(<20 min)']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Count'}, ax=ax)\n",
    "    ax.set_xlabel('Predicted State', fontsize=12)\n",
    "    ax.set_ylabel('Actual State', fontsize=12)\n",
    "    ax.set_title('Health State Classification', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_additional_analysis(y_test, y_pred, history):\n",
    "    \"\"\"Additional analysis plots\"\"\"\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Error by RUL range\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    bins = [0, 20, 40, 60, 80, 100]\n",
    "    bin_labels = ['0-20', '20-40', '40-60', '60-80', '80-100']\n",
    "    errors = np.abs(y_pred - y_test)\n",
    "    \n",
    "    binned_errors = []\n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (y_test >= bins[i]) & (y_test < bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            binned_errors.append(errors[mask].mean())\n",
    "        else:\n",
    "            binned_errors.append(0)\n",
    "    \n",
    "    ax1.bar(bin_labels, binned_errors, color='steelblue', edgecolor='black')\n",
    "    ax1.set_xlabel('RUL Range (minutes)', fontsize=10)\n",
    "    ax1.set_ylabel('Mean Absolute Error', fontsize=10)\n",
    "    ax1.set_title('Error by RUL Range', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Residual plot\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    residuals = y_pred - y_test\n",
    "    ax2.scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "    ax2.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('Predicted RUL', fontsize=10)\n",
    "    ax2.set_ylabel('Residuals', fontsize=10)\n",
    "    ax2.set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Percentile performance\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    percentiles = [10, 25, 50, 75, 90]\n",
    "    percentile_errors = [np.percentile(errors, p) for p in percentiles]\n",
    "    ax3.plot(percentiles, percentile_errors, 'o-', linewidth=2, markersize=8)\n",
    "    ax3.set_xlabel('Percentile', fontsize=10)\n",
    "    ax3.set_ylabel('Absolute Error', fontsize=10)\n",
    "    ax3.set_title('Error Percentiles', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Classification metrics\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    def classify_health(rul):\n",
    "        if rul > 60:\n",
    "            return 2\n",
    "        elif rul > 20:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    y_test_class = np.array([classify_health(r) for r in y_test])\n",
    "    y_pred_class = np.array([classify_health(r) for r in y_pred])\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "    precision = precision_score(y_test_class, y_pred_class, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test_class, y_pred_class, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_class, y_pred_class, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    metrics_values = [accuracy, precision, recall, f1]\n",
    "    colors_bar = ['#22c55e', '#3b82f6', '#f59e0b', '#8b5cf6']\n",
    "    \n",
    "    bars = ax4.bar(metrics_names, metrics_values, color=colors_bar, edgecolor='black')\n",
    "    ax4.set_ylabel('Score', fontsize=10)\n",
    "    ax4.set_title('Classification Metrics', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylim([0, 1.1])\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars, metrics_values):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Sample predictions\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    sample_length = min(100, len(y_test))\n",
    "    sample_indices = np.arange(sample_length)\n",
    "    ax5.plot(sample_indices, y_test[:sample_length], 'g-', label='Actual', linewidth=2, alpha=0.7)\n",
    "    ax5.plot(sample_indices, y_pred[:sample_length], 'b--', label='Predicted', linewidth=2, alpha=0.7)\n",
    "    ax5.axhline(20, color='r', linestyle=':', linewidth=2, alpha=0.5)\n",
    "    ax5.fill_between(sample_indices, 0, 20, alpha=0.1, color='red')\n",
    "    ax5.set_xlabel('Sample Index', fontsize=10)\n",
    "    ax5.set_ylabel('RUL (minutes)', fontsize=10)\n",
    "    ax5.set_title('Sample Predictions', fontsize=12, fontweight='bold')\n",
    "    ax5.legend(fontsize=8)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error by health state\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    healthy_errors = errors[y_test > 60]\n",
    "    warning_errors = errors[(y_test >= 20) & (y_test <= 60)]\n",
    "    critical_errors = errors[y_test < 20]\n",
    "    \n",
    "    box_data = [healthy_errors, warning_errors, critical_errors]\n",
    "    bp = ax6.boxplot(box_data, labels=['Healthy', 'Warning', 'Critical'], patch_artist=True)\n",
    "    \n",
    "    colors_box = ['#22c55e', '#f59e0b', '#ef4444']\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    ax6.set_ylabel('Absolute Error', fontsize=10)\n",
    "    ax6.set_title('Error by Health State', fontsize=12, fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    gap = np.array(val_loss) - np.array(train_loss)\n",
    "    \n",
    "    ax7.plot(epochs, gap, linewidth=2, color='purple')\n",
    "    ax7.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    ax7.fill_between(epochs, 0, gap, where=(gap > 0), alpha=0.3, color='red', label='Overfitting')\n",
    "    ax7.fill_between(epochs, 0, gap, where=(gap <= 0), alpha=0.3, color='green', label='Good fit')\n",
    "    ax7.set_xlabel('Epoch', fontsize=10)\n",
    "    ax7.set_ylabel('Val Loss - Train Loss', fontsize=10)\n",
    "    ax7.set_title('Overfitting Analysis', fontsize=12, fontweight='bold')\n",
    "    ax7.legend(fontsize=8)\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    ax8.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax8.axvline(errors.mean(), color='r', linestyle='--', linewidth=2, label=f'Mean: {errors.mean():.2f}')\n",
    "    ax8.set_xlabel('Absolute Error', fontsize=10)\n",
    "    ax8.set_ylabel('Frequency', fontsize=10)\n",
    "    ax8.set_title('Error Distribution', fontsize=12, fontweight='bold')\n",
    "    ax8.legend(fontsize=8)\n",
    "    ax8.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Loss comparison\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    final_epochs = min(50, len(history.history['loss']))\n",
    "    ax9.plot(range(1, final_epochs+1), train_loss[:final_epochs], label='Train', linewidth=2)\n",
    "    ax9.plot(range(1, final_epochs+1), val_loss[:final_epochs], label='Val', linewidth=2)\n",
    "    ax9.set_xlabel('Epoch', fontsize=10)\n",
    "    ax9.set_ylabel('Loss', fontsize=10)\n",
    "    ax9.set_title('Loss Convergence', fontsize=12, fontweight='bold')\n",
    "    ax9.legend(fontsize=8)\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('additional_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ==================== INFERENCE ====================\n",
    "def predict_maintenance(model, scaler_X, scaler_y, new_data, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Predict RUL for new sensor data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        scaler_X: Fitted feature scaler\n",
    "        scaler_y: Fitted target scaler\n",
    "        new_data: New sensor readings (shape: [sequence_length, num_features])\n",
    "        sequence_length: Length of input sequence\n",
    "    \n",
    "    Returns:\n",
    "        dict with RUL prediction and health state\n",
    "    \"\"\"\n",
    "    if len(new_data.shape) == 2:\n",
    "        new_data = new_data.reshape(1, sequence_length, -1)\n",
    "    \n",
    "    new_data_reshaped = new_data.reshape(-1, new_data.shape[-1])\n",
    "    new_data_scaled = scaler_X.transform(new_data_reshaped)\n",
    "    new_data_scaled = new_data_scaled.reshape(new_data.shape)\n",
    "    \n",
    "    rul_scaled = model.predict(new_data_scaled, verbose=0)\n",
    "    rul = scaler_y.inverse_transform(rul_scaled.reshape(-1, 1)).flatten()[0]\n",
    "    \n",
    "    if rul > 60:\n",
    "        health_state = \"Healthy\"\n",
    "        recommendation = \"Normal operation. Continue monitoring.\"\n",
    "    elif rul > 20:\n",
    "        health_state = \"Warning\"\n",
    "        recommendation = f\"Schedule maintenance within {int(rul)} minutes. Monitor closely.\"\n",
    "    else:\n",
    "        health_state = \"Critical\"\n",
    "        recommendation = f\"URGENT: Failure predicted in {int(rul)} minutes. Stop operation!\"\n",
    "    \n",
    "    result = {\n",
    "        'rul_minutes': rul,\n",
    "        'rul_hours': rul / 60,\n",
    "        'health_state': health_state,\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def save_synthetic_dataset(X, y, filename='synthetic_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Save synthetic dataset to CSV file\n",
    "    \n",
    "    Args:\n",
    "        X: Feature array (shape: [samples, features])\n",
    "        y: RUL labels (shape: [samples])\n",
    "        filename: Output CSV filename\n",
    "    \"\"\"\n",
    "    # Create column names\n",
    "    num_features = X.shape[1]\n",
    "    feature_columns = [f'feature_{i+1}' for i in range(num_features)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X, columns=feature_columns)\n",
    "    df['RUL'] = y\n",
    "    \n",
    "    # Add health state classification\n",
    "    def classify_health(rul):\n",
    "        if rul > 60:\n",
    "            return 'Healthy'\n",
    "        elif rul > 20:\n",
    "            return 'Warning'\n",
    "        else:\n",
    "            return 'Critical'\n",
    "    \n",
    "    df['Health_State'] = df['RUL'].apply(classify_health)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✓ Synthetic dataset saved to '{filename}'\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {len(feature_columns)} features + RUL + Health_State\")\n",
    "    print(f\"  Health distribution:\")\n",
    "    print(df['Health_State'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_sequences_dataset(X_seq, y_seq, filename='synthetic_sequences.csv'):\n",
    "    \"\"\"\n",
    "    Save sequence dataset (for LSTM input) to CSV\n",
    "    \n",
    "    Args:\n",
    "        X_seq: Sequence array (shape: [samples, sequence_length, features])\n",
    "        y_seq: RUL labels (shape: [samples])\n",
    "        filename: Output CSV filename\n",
    "    \"\"\"\n",
    "    num_samples, seq_length, num_features = X_seq.shape\n",
    "    \n",
    "    # Reshape to 2D: each row is one sequence flattened\n",
    "    X_flattened = X_seq.reshape(num_samples, -1)\n",
    "    \n",
    "    # Create column names: feature_timestep_featurenum\n",
    "    columns = []\n",
    "    for t in range(seq_length):\n",
    "        for f in range(num_features):\n",
    "            columns.append(f't{t}_f{f}')\n",
    "    \n",
    "    df = pd.DataFrame(X_flattened, columns=columns)\n",
    "    df['RUL'] = y_seq\n",
    "    \n",
    "    # Add health state\n",
    "    def classify_health(rul):\n",
    "        if rul > 60:\n",
    "            return 'Healthy'\n",
    "        elif rul > 20:\n",
    "            return 'Warning'\n",
    "        else:\n",
    "            return 'Critical'\n",
    "    \n",
    "    df['Health_State'] = df['RUL'].apply(classify_health)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✓ Sequence dataset saved to '{filename}'\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Original sequence shape: ({num_samples}, {seq_length}, {num_features})\")\n",
    "    print(f\"  Flattened to: ({num_samples}, {seq_length * num_features})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_train_test_split_datasets(X_train, X_test, y_train, y_test, prefix='data'):\n",
    "    \"\"\"\n",
    "    Save train and test sets separately\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Feature arrays\n",
    "        y_train, y_test: Label arrays\n",
    "        prefix: Filename prefix\n",
    "    \"\"\"\n",
    "    # Training set\n",
    "    train_df = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])\n",
    "    train_df['RUL'] = y_train\n",
    "    train_df.to_csv(f'{prefix}_train.csv', index=False)\n",
    "    print(f\"✓ Training set saved: {train_df.shape}\")\n",
    "    \n",
    "    # Test set\n",
    "    test_df = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])\n",
    "    test_df['RUL'] = y_test\n",
    "    test_df.to_csv(f'{prefix}_test.csv', index=False)\n",
    "    print(f\"✓ Test set saved: {test_df.shape}\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREDICTIVE MAINTENANCE SYSTEM - CNN-LSTM MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n[1/8] Loading data...\")\n",
    "    print(\"Note: Using mock data for demonstration. Replace with actual data loading.\")\n",
    "    \n",
    "    print(\"\\n[2/8] Preprocessing data (resampling to 1kHz)...\")\n",
    "    \n",
    "    print(\"\\n[3/8] Extracting features from windowed data...\")\n",
    "    \n",
    "    num_features = 100\n",
    "    feature_data = load_actual_data_example()\n",
    "    \n",
    "    print(\"\\n[4/8] Generating synthetic degradation paths...\")\n",
    "    X, y = create_synthetic_dataset(feature_data)\n",
    "    print(\"\\nSaving synthetic dataset...\")\n",
    "    save_synthetic_dataset(X, y, 'synthetic_dataset.csv')\n",
    "\n",
    "    \n",
    "    print(\"\\n[5/8] Creating sequences for LSTM...\")\n",
    "    sequence_length = 10\n",
    "    X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "    print(f\"Sequence shape: {X_seq.shape}, Target shape: {y_seq.shape}\")\n",
    "    print(\"\\nSaving sequence dataset...\")\n",
    "    save_sequences_dataset(X_seq, y_seq, 'synthetic_sequences.csv')\n",
    "    \n",
    "    scaler_X = StandardScaler()\n",
    "    X_seq_reshaped = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    X_seq_scaled = scaler_X.fit_transform(X_seq_reshaped)\n",
    "    X_seq_scaled = X_seq_scaled.reshape(X_seq.shape)\n",
    "    \n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_seq_scaled = scaler_y.fit_transform(y_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_seq_scaled, y_seq_scaled, test_size=0.3, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    print(\"\\n[6/8] Building CNN-LSTM model...\")\n",
    "    model = build_cnn_lstm_model(X_train.shape[1:])\n",
    "    print(model.summary())\n",
    "    \n",
    "    print(\"\\n[7/8] Training model...\")\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    print(\"\\n[8/8] Evaluating and visualizing results...\")\n",
    "    y_pred_scaled, metrics = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    metrics_original = {\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test_original, y_pred_original)),\n",
    "        'mae': mean_absolute_error(y_test_original, y_pred_original),\n",
    "        'r2': r2_score(y_test_original, y_pred_original)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMetrics on Original Scale:\")\n",
    "    print(f\"RMSE: {metrics_original['rmse']:.2f} minutes\")\n",
    "    print(f\"MAE: {metrics_original['mae']:.2f} minutes\")\n",
    "    print(f\"R² Score: {metrics_original['r2']:.4f}\")\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    plot_training_history(history)\n",
    "    plot_predictions(y_test_original, y_pred_original, metrics_original)\n",
    "    plot_rul_progression(y_test_original, y_pred_original)\n",
    "    plot_confusion_matrix()\n",
    "    plot_additional_analysis(y_test_original, y_pred_original, history)\n",
    "    \n",
    "    print(\"\\nSaving model and scalers...\")\n",
    "    model.save('predictive_maintenance_model.h5')\n",
    "    joblib.dump(scaler_X, 'scaler_X.pkl')\n",
    "    joblib.dump(scaler_y, 'scaler_y.pkl')\n",
    "    print(\"✓ Model saved as 'predictive_maintenance_model.h5'\")\n",
    "    print(\"✓ Scalers saved as 'scaler_X.pkl' and 'scaler_y.pkl'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return model, history, metrics_original, (y_test_original, y_pred_original), scaler_X, scaler_y\n",
    "\n",
    "# ==================== DEMO INFERENCE ====================\n",
    "def demo_inference():\n",
    "    \"\"\"Demonstrate inference with trained model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INFERENCE DEMO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        model = keras.models.load_model('predictive_maintenance_model.h5')\n",
    "        scaler_X = joblib.load('scaler_X.pkl')\n",
    "        scaler_y = joblib.load('scaler_y.pkl')\n",
    "        \n",
    "        print(\"✓ Model and scalers loaded successfully!\")\n",
    "        \n",
    "        sequence_length = 10\n",
    "        num_features = 100\n",
    "        new_data = np.random.randn(sequence_length, num_features)\n",
    "        \n",
    "        print(f\"\\nProcessing new sensor data (shape: {new_data.shape})...\")\n",
    "        \n",
    "        result = predict_maintenance(model, scaler_X, scaler_y, new_data, sequence_length)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"MAINTENANCE PREDICTION RESULTS\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"Remaining Useful Life: {result['rul_minutes']:.1f} minutes ({result['rul_hours']:.2f} hours)\")\n",
    "        print(f\"Health State: {result['health_state']}\")\n",
    "        print(f\"Recommendation: {result['recommendation']}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        print(\"Please run the main training pipeline first.\")\n",
    "\n",
    "# ==================== ACTUAL DATA LOADING EXAMPLE ====================\n",
    "def load_actual_data_example():\n",
    "    \"\"\"\n",
    "    Example of how to load and process actual CSV files\n",
    "    Uncomment and modify this when you have real data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING ACTUAL DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    raw_data = load_all_data()\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_data = {}\n",
    "    for condition in ['Normal', 'BPFI_03', 'BPFI_10', 'BPFI_30']:\n",
    "        print(f\"\\nProcessing {condition}...\")\n",
    "        processed_data[condition] = {\n",
    "            'temp': preprocess_dataframe(raw_data['current_temp'][condition], 'current_temp'),\n",
    "            'vib': preprocess_dataframe(raw_data['vibration'][condition], 'vibration')\n",
    "        }\n",
    "    \n",
    "    # Extract features\n",
    "    feature_data = {}\n",
    "    for condition in ['Normal', 'BPFI_03', 'BPFI_10', 'BPFI_30']:\n",
    "        print(f\"\\nExtracting features for {condition}...\")\n",
    "        feature_data[condition] = create_windowed_features(\n",
    "            processed_data[condition]['temp'],\n",
    "            processed_data[condition]['vib'],\n",
    "            config.WINDOW_SIZE,\n",
    "            config.OVERLAP\n",
    "        )\n",
    "        print(f\"{condition}: {feature_data[condition].shape[0]} windows, {feature_data[condition].shape[1]} features\")\n",
    "    \n",
    "    return feature_data\n",
    "\n",
    "# ==================== RUN ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Run main training pipeline\n",
    "    model, history, metrics, predictions, scaler_X, scaler_y = main()\n",
    "    \n",
    "    # Demonstrate inference\n",
    "    demo_inference()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL OPERATIONS COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  - predictive_maintenance_model.h5 (trained model)\")\n",
    "    print(\"  - scaler_X.pkl, scaler_y.pkl (data scalers)\")\n",
    "    print(\"  - training_history.png\")\n",
    "    print(\"  - predictions_analysis.png\")\n",
    "    print(\"  - rul_progression.png\")\n",
    "    print(\"  - confusion_matrix.png\")\n",
    "    print(\"  - additional_analysis.png\")\n",
    "    print(\"\\nTo use with your actual data:\")\n",
    "    print(\"  1. Place CSV files in 'current_temp/' and 'vibration/' folders\")\n",
    "    print(\"  2. Uncomment load_actual_data_example() function call\")\n",
    "    print(\"  3. Replace mock data with: feature_data = load_actual_data_example()\")\n",
    "    print(\"\\nFor real-time predictions:\")\n",
    "    print(\"  result = predict_maintenance(model, scaler_X, scaler_y, new_sensor_data)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e3d45-f96f-4e2b-8324-90f675fd6f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
